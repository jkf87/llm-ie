"""
Relation Inference Sub-agents for Knowledge Graph Generation
ê´€ê³„ ì¶”ë¡ ì„ ìœ„í•œ ì„œë¸Œ ì—ì´ì „íŠ¸ë“¤
"""

import json
import logging
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
import re


logger = logging.getLogger(__name__)


class RelationPromptAgent:
    """
    ê´€ê³„ ì¶”ë¡  í”„ë¡¬í”„íŠ¸ ìƒì„±ì„ ìœ„í•œ Sub-agent
    ì»¨í…ìŠ¤íŠ¸ì™€ ì—”í‹°í‹° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ê´€ê³„ ì¶”ë¡  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±
    """
    
    def __init__(self, llm_engine=None):
        self.llm_engine = llm_engine
        
        # ë„ë©”ì¸ë³„ ê´€ê³„ í…œí”Œë¦¿
        self.domain_templates = {
            "research_paper": {
                "name": "ì—°êµ¬ ë…¼ë¬¸",
                "description": "í•™ìˆ  ë…¼ë¬¸ì˜ ì—”í‹°í‹° ê°„ ê´€ê³„",
                "common_relations": [
                    "STUDIES", "ANALYZES", "COMPARES", "USES", "MEASURES", 
                    "EVALUATES", "PROPOSES", "IMPLEMENTS", "CITES", "EXTENDS"
                ],
                "relation_patterns": {
                    "MODEL-MODEL": ["COMPARES_WITH", "SIMILAR_TO", "BETTER_THAN"],
                    "RESEARCH-MODEL": ["STUDIES", "ANALYZES", "EVALUATES"],
                    "RESEARCH-SURVEY": ["USES", "IMPLEMENTS", "APPLIES"],
                    "MODEL-TECHNOLOGY": ["IS_TYPE_OF", "IMPLEMENTS", "USES"],
                    "DOCUMENT-RESEARCH": ["DESCRIBES", "PRESENTS", "REPORTS"]
                }
            },
            "technical_document": {
                "name": "ê¸°ìˆ  ë¬¸ì„œ",
                "description": "ê¸°ìˆ  ë¬¸ì„œì˜ ì»´í¬ë„ŒíŠ¸ ê°„ ê´€ê³„",
                "common_relations": [
                    "IMPLEMENTS", "USES", "DEPENDS_ON", "PART_OF", "CONTAINS",
                    "CONNECTS_TO", "CONFIGURES", "MANAGES", "PROCESSES"
                ],
                "relation_patterns": {
                    "COMPONENT-COMPONENT": ["CONNECTS_TO", "DEPENDS_ON", "PART_OF"],
                    "SYSTEM-COMPONENT": ["CONTAINS", "MANAGES", "USES"],
                    "PROCESS-DATA": ["PROCESSES", "TRANSFORMS", "ANALYZES"],
                    "API-SERVICE": ["PROVIDES", "IMPLEMENTS", "EXPOSES"]
                }
            },
            "business_document": {
                "name": "ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ",
                "description": "ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ì™€ ì¡°ì§ ê°„ ê´€ê³„",
                "common_relations": [
                    "MANAGES", "REPORTS_TO", "COLLABORATES_WITH", "RESPONSIBLE_FOR",
                    "APPROVES", "MONITORS", "SUPPORTS", "DELIVERS", "CONTRACTS_WITH"
                ],
                "relation_patterns": {
                    "PERSON-PERSON": ["REPORTS_TO", "COLLABORATES_WITH", "MANAGES"],
                    "ORGANIZATION-PERSON": ["EMPLOYS", "CONTRACTS_WITH", "PARTNERS_WITH"],
                    "PROCESS-ORGANIZATION": ["MANAGED_BY", "EXECUTED_BY", "OWNED_BY"],
                    "PRODUCT-CUSTOMER": ["USED_BY", "PURCHASED_BY", "REQUESTED_BY"]
                }
            },
            "general": {
                "name": "ì¼ë°˜",
                "description": "ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ì˜ ì—”í‹°í‹° ê°„ ê´€ê³„",
                "common_relations": [
                    "RELATED_TO", "PART_OF", "CONTAINS", "DESCRIBES", "MENTIONS",
                    "REFERS_TO", "ASSOCIATED_WITH", "LOCATED_IN", "OCCURS_IN"
                ],
                "relation_patterns": {
                    "ENTITY-ENTITY": ["RELATED_TO", "ASSOCIATED_WITH", "CONNECTED_TO"],
                    "DOCUMENT-ENTITY": ["MENTIONS", "DESCRIBES", "DISCUSSES"],
                    "LOCATION-ENTITY": ["CONTAINS", "LOCATED_IN", "HOSTS"],
                    "TIME-EVENT": ["OCCURS_IN", "DURING", "BEFORE", "AFTER"]
                }
            }
        }
    
    def generate_relation_prompt(self, 
                                text: str, 
                                entities: List[Dict], 
                                user_context: str = "",
                                domain: str = "general",
                                additional_guidelines: str = "",
                                max_relations: int = 20) -> Dict[str, Any]:
        """
        ê´€ê³„ ì¶”ë¡ ì„ ìœ„í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            entities: ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            user_context: ì‚¬ìš©ì ì œê³µ ì»¨í…ìŠ¤íŠ¸
            domain: ë„ë©”ì¸ ìœ í˜•
            additional_guidelines: ì¶”ê°€ ê°€ì´ë“œë¼ì¸
            max_relations: ìµœëŒ€ ê´€ê³„ ìˆ˜
            
        Returns:
            í”„ë¡¬í”„íŠ¸ ìƒì„± ê²°ê³¼
        """
        try:
            # ë„ë©”ì¸ í…œí”Œë¦¿ ì„ íƒ
            domain_template = self.domain_templates.get(domain, self.domain_templates["general"])
            
            # ì—”í‹°í‹° íƒ€ì… ë¶„ì„
            entity_types = {}
            entity_pairs = []
            
            for entity in entities:
                entity_type = entity.get('type', 'UNKNOWN')
                if entity_type not in entity_types:
                    entity_types[entity_type] = []
                entity_types[entity_type].append(entity)
            
            # ì—”í‹°í‹° ìŒ ìƒì„± (ëª¨ë“  ì¡°í•©)
            for i, entity1 in enumerate(entities):
                for j, entity2 in enumerate(entities):
                    if i != j:  # ìê¸° ìì‹ ê³¼ì˜ ê´€ê³„ëŠ” ì œì™¸
                        pair_key = f"{entity1.get('type', 'UNKNOWN')}-{entity2.get('type', 'UNKNOWN')}"
                        entity_pairs.append({
                            'entity1': entity1,
                            'entity2': entity2,
                            'pair_type': pair_key
                        })
            
            # ê´€ê³„ íŒ¨í„´ ì¶”ì²œ
            recommended_relations = self._get_recommended_relations(entity_types, domain_template)
            
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
            base_prompt = self._create_base_prompt(
                text, entities, domain_template, recommended_relations,
                user_context, additional_guidelines, max_relations
            )
            
            # LLMì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™” (ì„ íƒì )
            optimized_prompt = base_prompt
            if self.llm_engine:
                optimized_prompt = self._optimize_prompt_with_llm(
                    base_prompt, text, entities, user_context
                )
            
            return {
                'success': True,
                'base_prompt': base_prompt,
                'optimized_prompt': optimized_prompt,
                'domain': domain,
                'domain_info': domain_template,
                'entity_types': entity_types,
                'entity_pairs_count': len(entity_pairs),
                'recommended_relations': recommended_relations,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error generating relation prompt: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _get_recommended_relations(self, entity_types: Dict, domain_template: Dict) -> List[str]:
        """ì—”í‹°í‹° íƒ€ì… ê¸°ë°˜ ì¶”ì²œ ê´€ê³„ ìƒì„±"""
        recommended = set(domain_template['common_relations'])
        
        # ì—”í‹°í‹° íƒ€ì… ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
        for type1 in entity_types.keys():
            for type2 in entity_types.keys():
                if type1 != type2:
                    pair_key = f"{type1}-{type2}"
                    if pair_key in domain_template['relation_patterns']:
                        recommended.update(domain_template['relation_patterns'][pair_key])
        
        return sorted(list(recommended))
    
    def _create_base_prompt(self, text: str, entities: List[Dict], domain_template: Dict,
                           recommended_relations: List[str], user_context: str,
                           additional_guidelines: str, max_relations: int) -> str:
        """ê¸°ë³¸ ê´€ê³„ ì¶”ë¡  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ì—”í‹°í‹° ì •ë³´ í¬ë§·íŒ…
        entity_info = []
        for entity in entities:
            entity_info.append(f"- ID: {entity['id']}, í…ìŠ¤íŠ¸: \"{entity['text']}\", íƒ€ì…: {entity['type']}")
        
        # ê´€ê³„ íƒ€ì… ì„¤ëª…
        relation_descriptions = {
            "STUDIES": "Aê°€ Bë¥¼ ì—°êµ¬/ì¡°ì‚¬í•¨",
            "ANALYZES": "Aê°€ Bë¥¼ ë¶„ì„í•¨",
            "COMPARES": "Aê°€ Bì™€ ë¹„êµë¨",
            "USES": "Aê°€ Bë¥¼ ì‚¬ìš©/í™œìš©í•¨",
            "MEASURES": "Aê°€ Bë¥¼ ì¸¡ì •í•¨",
            "EVALUATES": "Aê°€ Bë¥¼ í‰ê°€í•¨",
            "IMPLEMENTS": "Aê°€ Bë¥¼ êµ¬í˜„í•¨",
            "IS_TYPE_OF": "Aê°€ Bì˜ ì¼ì¢…ì„",
            "PART_OF": "Aê°€ Bì˜ ì¼ë¶€ì„",
            "CONTAINS": "Aê°€ Bë¥¼ í¬í•¨í•¨",
            "DESCRIBES": "Aê°€ Bë¥¼ ì„¤ëª…/ê¸°ìˆ í•¨",
            "REFERS_TO": "Aê°€ Bë¥¼ ì°¸ì¡°/ì–¸ê¸‰í•¨",
            "RELATED_TO": "Aê°€ Bì™€ ê´€ë ¨ë¨",
            "SIMILAR_TO": "Aê°€ Bì™€ ìœ ì‚¬í•¨",
            "BETTER_THAN": "Aê°€ Bë³´ë‹¤ ìš°ìˆ˜í•¨",
            "DEPENDS_ON": "Aê°€ Bì— ì˜ì¡´í•¨",
            "CONNECTS_TO": "Aê°€ Bì™€ ì—°ê²°ë¨",
            "MANAGES": "Aê°€ Bë¥¼ ê´€ë¦¬í•¨",
            "REPORTS_TO": "Aê°€ Bì—ê²Œ ë³´ê³ í•¨"
        }
        
        relation_list = []
        for rel in recommended_relations:
            desc = relation_descriptions.get(rel, f"Aê°€ Bì™€ {rel.lower().replace('_', ' ')} ê´€ê³„")
            relation_list.append(f"- {rel}: {desc}")
        
        prompt = f"""# ì—”í‹°í‹° ê´€ê³„ ì¶”ë¡  ì‘ì—…

## ğŸ“„ ë¬¸ì„œ ì •ë³´
**ë„ë©”ì¸**: {domain_template['name']} ({domain_template['description']})
**ì›ë³¸ í…ìŠ¤íŠ¸**: 
```
{text}
```

## ğŸ¯ ì¶”ì¶œëœ ì—”í‹°í‹°ë“¤
{chr(10).join(entity_info)}

## ğŸ’¡ ì‚¬ìš©ì ì œê³µ ì»¨í…ìŠ¤íŠ¸
{user_context if user_context.strip() else "ì—†ìŒ"}

## ğŸ“‹ ì‘ì—… ì§€ì‹œì‚¬í•­
ìœ„ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ì—”í‹°í‹°ë“¤ ì‚¬ì´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ìµœëŒ€ {max_relations}ê°œì˜ ê´€ê³„ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

### ğŸ”— ì¶”ì²œ ê´€ê³„ íƒ€ì…ë“¤
{chr(10).join(relation_list)}

### ğŸ“ ê´€ê³„ ì¶”ë¡  ì›ì¹™
1. **í…ìŠ¤íŠ¸ ê¸°ë°˜**: ë°˜ë“œì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ê±°ë‚˜ ê°•í•˜ê²Œ ì•”ì‹œë˜ëŠ” ê´€ê³„ë§Œ ì¶”ë¡ 
2. **ì‹ ë¢°ë„ í‰ê°€**: ê° ê´€ê³„ì— ëŒ€í•´ 0.0~1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„ ì ìˆ˜ ë¶€ì—¬
3. **ì¤‘ìš”ë„ ìš°ì„ **: ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ê³¼ ê´€ë ¨ëœ ê´€ê³„ë¥¼ ìš°ì„  ì„ íƒ
4. **ì¤‘ë³µ ë°©ì§€**: ë™ì¼í•œ ì—”í‹°í‹° ìŒì— ëŒ€í•´ ê°€ì¥ ì ì ˆí•œ í•˜ë‚˜ì˜ ê´€ê³„ë§Œ ì„ íƒ
5. **ë°©í–¥ì„± ê³ ë ¤**: ê´€ê³„ì˜ ë°©í–¥ì„±ì´ ì¤‘ìš”í•œ ê²½ìš° subjectì™€ object ìˆœì„œ ì£¼ì˜

{f"### ğŸ“Œ ì¶”ê°€ ê°€ì´ë“œë¼ì¸{chr(10)}{additional_guidelines}" if additional_guidelines.strip() else ""}

## ğŸ“¤ ì¶œë ¥ í˜•ì‹
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
[
    {{
        "subject": "ì—”í‹°í‹°_ID1",
        "type": "ê´€ê³„_íƒ€ì…",
        "object": "ì—”í‹°í‹°_ID2",
        "confidence": 0.85,
        "explanation": "ê´€ê³„ ì„¤ì • ê·¼ê±° (í…ìŠ¤íŠ¸ì˜ íŠ¹ì • ë¶€ë¶„ ì¸ìš©)",
        "text_evidence": "ê´€ê³„ë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ì›ë¬¸ êµ¬ì ˆ"
    }}
]
```

**ì¤‘ìš”**: JSON ë°°ì—´ í˜•ì‹ë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

        return prompt
    
    def _optimize_prompt_with_llm(self, base_prompt: str, text: str, 
                                entities: List[Dict], user_context: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        try:
            optimization_prompt = f"""
ë‹¤ìŒ ê´€ê³„ ì¶”ë¡  í”„ë¡¬í”„íŠ¸ë¥¼ ë” íš¨ê³¼ì ì´ê³  ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ ê°œì„ í•´ì£¼ì„¸ìš”.

**ì›ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì„±**: {len(text)} ê¸€ì, {len(entities)}ê°œ ì—”í‹°í‹°
**ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸**: {user_context or "ì—†ìŒ"}

**ê°œì„ í•  í”„ë¡¬í”„íŠ¸**:
{base_prompt}

**ê°œì„  ìš”ì²­ì‚¬í•­**:
1. ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§€ì‹œì‚¬í•­ ì œê³µ
2. í…ìŠ¤íŠ¸ì˜ íŠ¹ì„±ì— ë§ëŠ” ê´€ê³„ íƒ€ì… ê°•ì¡°
3. ì˜¤ë¥˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆëŠ” ì¶”ê°€ ì•ˆë‚´ í¬í•¨
4. JSON ì¶œë ¥ í˜•ì‹ì˜ ëª…í™•ì„± í–¥ìƒ

ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
"""
            
            messages = [{"role": "user", "content": optimization_prompt}]
            response_generator = self.llm_engine.chat_completion(messages, stream=False)
            
            optimized_text = ""
            for response in response_generator:
                if response.get('type') == 'content':
                    optimized_text += response.get('text', '')
            
            # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ê²€ì¦
            if len(optimized_text.strip()) > len(base_prompt) * 0.5:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                return optimized_text.strip()
            else:
                logger.warning("Optimized prompt too short, using base prompt")
                return base_prompt
                
        except Exception as e:
            logger.warning(f"Failed to optimize prompt with LLM: {e}")
            return base_prompt


class RelationEvaluatorAgent:
    """
    ê´€ê³„ ì¶”ë¡  ê²°ê³¼ í‰ê°€ë¥¼ ìœ„í•œ Sub-agent
    LLMì´ ì¶”ë¡ í•œ ê´€ê³„ë“¤ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì œì‹œ
    """
    
    def __init__(self, llm_engine=None):
        self.llm_engine = llm_engine
        
        # í‰ê°€ ê¸°ì¤€
        self.evaluation_criteria = {
            "text_support": {
                "name": "í…ìŠ¤íŠ¸ ê·¼ê±°ì„±",
                "description": "ê´€ê³„ê°€ ì›ë³¸ í…ìŠ¤íŠ¸ì— ì˜í•´ ë’·ë°›ì¹¨ë˜ëŠ” ì •ë„",
                "weight": 0.3
            },
            "logical_consistency": {
                "name": "ë…¼ë¦¬ì  ì¼ê´€ì„±",
                "description": "ê´€ê³„ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ë˜ê³  ëª¨ìˆœì´ ì—†ëŠ” ì •ë„",
                "weight": 0.25
            },
            "semantic_accuracy": {
                "name": "ì˜ë¯¸ì  ì •í™•ì„±",
                "description": "ê´€ê³„ íƒ€ì…ì´ ì—”í‹°í‹° ê°„ì˜ ì‹¤ì œ ê´€ê³„ë¥¼ ì •í™•íˆ í‘œí˜„í•˜ëŠ” ì •ë„",
                "weight": 0.25
            },
            "relevance": {
                "name": "ê´€ë ¨ì„±",
                "description": "ê´€ê³„ê°€ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€",
                "weight": 0.2
            }
        }
        
        # í’ˆì§ˆ ì„ê³„ê°’
        self.quality_thresholds = {
            "excellent": 0.85,
            "good": 0.7,
            "acceptable": 0.55,
            "poor": 0.0
        }
    
    def evaluate_relations(self, 
                          relations: List[Dict], 
                          text: str, 
                          entities: List[Dict],
                          user_context: str = "",
                          domain: str = "general") -> Dict[str, Any]:
        """
        ì¶”ë¡ ëœ ê´€ê³„ë“¤ì˜ í’ˆì§ˆ í‰ê°€
        
        Args:
            relations: ì¶”ë¡ ëœ ê´€ê³„ ë¦¬ìŠ¤íŠ¸
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            entities: ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
            user_context: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            domain: ë„ë©”ì¸ ìœ í˜•
            
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        try:
            if not relations:
                return {
                    'success': True,
                    'overall_score': 0.0,
                    'quality_level': 'poor',
                    'relation_scores': [],
                    'recommendations': ["ê´€ê³„ê°€ ì¶”ë¡ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."],
                    'approved_relations': [],
                    'rejected_relations': [],
                    'needs_improvement': True,
                    'timestamp': datetime.now().isoformat()
                }
            
            # ê°œë³„ ê´€ê³„ í‰ê°€
            relation_evaluations = []
            approved_relations = []
            rejected_relations = []
            
            for i, relation in enumerate(relations):
                evaluation = self._evaluate_single_relation(
                    relation, text, entities, user_context
                )
                evaluation['relation_index'] = i
                relation_evaluations.append(evaluation)
                
                if evaluation['approved']:
                    approved_relations.append(relation)
                else:
                    rejected_relations.append({
                        'relation': relation,
                        'rejection_reason': evaluation['issues']
                    })
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            if relation_evaluations:
                overall_score = sum(eval['score'] for eval in relation_evaluations) / len(relation_evaluations)
            else:
                overall_score = 0.0
            
            # í’ˆì§ˆ ìˆ˜ì¤€ ê²°ì •
            quality_level = self._determine_quality_level(overall_score)
            
            # LLM ê¸°ë°˜ ì¶”ê°€ í‰ê°€ (ì„ íƒì )
            llm_evaluation = {}
            if self.llm_engine:
                llm_evaluation = self._evaluate_with_llm(
                    relations, text, entities, user_context, relation_evaluations
                )
            
            # ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_recommendations(
                relation_evaluations, overall_score, quality_level, llm_evaluation
            )
            
            # ê°œì„  í•„ìš” ì—¬ë¶€ ê²°ì •
            needs_improvement = overall_score < self.quality_thresholds['good']
            
            return {
                'success': True,
                'overall_score': overall_score,
                'quality_level': quality_level,
                'relation_scores': relation_evaluations,
                'approved_relations': approved_relations,
                'rejected_relations': rejected_relations,
                'recommendations': recommendations,
                'llm_evaluation': llm_evaluation,
                'needs_improvement': needs_improvement,
                'evaluation_criteria': self.evaluation_criteria,
                'statistics': {
                    'total_relations': len(relations),
                    'approved_count': len(approved_relations),
                    'rejected_count': len(rejected_relations),
                    'approval_rate': len(approved_relations) / len(relations) if relations else 0
                },
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error evaluating relations: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _evaluate_single_relation(self, relation: Dict, text: str, 
                                 entities: List[Dict], user_context: str) -> Dict[str, Any]:
        """ê°œë³„ ê´€ê³„ í‰ê°€"""
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['subject', 'type', 'object']
        missing_fields = [field for field in required_fields if field not in relation]
        
        if missing_fields:
            return {
                'score': 0.0,
                'approved': False,
                'issues': [f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {', '.join(missing_fields)}"],
                'criteria_scores': {criterion: 0.0 for criterion in self.evaluation_criteria}
            }
        
        # ì—”í‹°í‹° ì¡´ì¬ ê²€ì¦
        entity_ids = [e['id'] for e in entities]
        if relation['subject'] not in entity_ids:
            return {
                'score': 0.0,
                'approved': False,
                'issues': [f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” subject ì—”í‹°í‹°: {relation['subject']}"],
                'criteria_scores': {criterion: 0.0 for criterion in self.evaluation_criteria}
            }
        
        if relation['object'] not in entity_ids:
            return {
                'score': 0.0,
                'approved': False,
                'issues': [f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” object ì—”í‹°í‹°: {relation['object']}"],
                'criteria_scores': {criterion: 0.0 for criterion in self.evaluation_criteria}
            }
        
        # ê° ê¸°ì¤€ë³„ í‰ê°€
        criteria_scores = {}
        issues = []
        
        # 1. í…ìŠ¤íŠ¸ ê·¼ê±°ì„± í‰ê°€
        text_support_score = self._evaluate_text_support(relation, text)
        criteria_scores['text_support'] = text_support_score
        
        if text_support_score < 0.3:
            issues.append("í…ìŠ¤íŠ¸ì—ì„œ ê´€ê³„ë¥¼ ë’·ë°›ì¹¨í•˜ëŠ” ê·¼ê±°ê°€ ë¶€ì¡±í•¨")
        
        # 2. ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€
        logical_score = self._evaluate_logical_consistency(relation, entities)
        criteria_scores['logical_consistency'] = logical_score
        
        if logical_score < 0.3:
            issues.append("ê´€ê³„ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ë¶€ì¡±í•¨")
        
        # 3. ì˜ë¯¸ì  ì •í™•ì„± í‰ê°€
        semantic_score = self._evaluate_semantic_accuracy(relation, entities)
        criteria_scores['semantic_accuracy'] = semantic_score
        
        if semantic_score < 0.3:
            issues.append("ê´€ê³„ íƒ€ì…ì´ ì—”í‹°í‹° ê°„ì˜ ì‹¤ì œ ê´€ê³„ë¥¼ ë¶€ì •í™•í•˜ê²Œ í‘œí˜„í•¨")
        
        # 4. ê´€ë ¨ì„± í‰ê°€
        relevance_score = self._evaluate_relevance(relation, text, user_context)
        criteria_scores['relevance'] = relevance_score
        
        if relevance_score < 0.3:
            issues.append("ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ê³¼ ê´€ë ¨ì„±ì´ ë‚®ìŒ")
        
        # ê°€ì¤‘ í‰ê·  ì ìˆ˜ ê³„ì‚°
        total_score = sum(
            score * self.evaluation_criteria[criterion]['weight']
            for criterion, score in criteria_scores.items()
        )
        
        # ìŠ¹ì¸ ì—¬ë¶€ ê²°ì •
        approved = total_score >= self.quality_thresholds['acceptable'] and len(issues) == 0
        
        return {
            'score': total_score,
            'approved': approved,
            'issues': issues,
            'criteria_scores': criteria_scores,
            'confidence': relation.get('confidence', 0.5)
        }
    
    def _evaluate_text_support(self, relation: Dict, text: str) -> float:
        """í…ìŠ¤íŠ¸ ê·¼ê±°ì„± í‰ê°€"""
        try:
            # ê´€ê³„ì— text_evidenceê°€ ìˆëŠ”ì§€ í™•ì¸
            text_evidence = relation.get('text_evidence', '')
            explanation = relation.get('explanation', '')
            
            # í…ìŠ¤íŠ¸ ì¦ê±°ê°€ ì‹¤ì œë¡œ ì›ë¬¸ì— ìˆëŠ”ì§€ í™•ì¸
            if text_evidence and text_evidence.strip() in text:
                return 0.8
            
            # explanationì´ êµ¬ì²´ì ì¸ì§€ í™•ì¸
            if explanation and len(explanation.strip()) > 10:
                return 0.6
            
            # confidence ê°’ì´ ìˆìœ¼ë©´ ì°¸ê³ 
            confidence = relation.get('confidence', 0.5)
            if confidence > 0.7:
                return 0.5
            
            return 0.3
            
        except Exception:
            return 0.2
    
    def _evaluate_logical_consistency(self, relation: Dict, entities: List[Dict]) -> float:
        """ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€"""
        try:
            # ìê¸° ìì‹ ê³¼ì˜ ê´€ê³„ ì²´í¬
            if relation['subject'] == relation['object']:
                return 0.1
            
            # ì—”í‹°í‹° íƒ€ì… ê¸°ë°˜ ê´€ê³„ì˜ ì ì ˆì„± ì²´í¬
            subject_entity = next(e for e in entities if e['id'] == relation['subject'])
            object_entity = next(e for e in entities if e['id'] == relation['object'])
            
            subject_type = subject_entity.get('type', 'UNKNOWN')
            object_type = object_entity.get('type', 'UNKNOWN')
            relation_type = relation['type']
            
            # ì¼ë¶€ ê´€ê³„ íƒ€ì…ì— ëŒ€í•œ ê¸°ë³¸ ë…¼ë¦¬ ì²´í¬
            if relation_type == 'IS_TYPE_OF' and subject_type == object_type:
                return 0.3  # ê°™ì€ íƒ€ì…ë¼ë¦¬ëŠ” IS_TYPE_OFê°€ ë¶€ì ì ˆí•  ìˆ˜ ìˆìŒ
            
            if relation_type in ['PART_OF', 'CONTAINS'] and subject_type == object_type:
                return 0.6  # ê°™ì€ íƒ€ì…ë¼ë¦¬ë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì£¼ì˜ í•„ìš”
            
            return 0.8  # ê¸°ë³¸ì ìœ¼ë¡œëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœ ê²ƒìœ¼ë¡œ ê°€ì •
            
        except Exception:
            return 0.4
    
    def _evaluate_semantic_accuracy(self, relation: Dict, entities: List[Dict]) -> float:
        """ì˜ë¯¸ì  ì •í™•ì„± í‰ê°€"""
        try:
            relation_type = relation['type']
            confidence = relation.get('confidence', 0.5)
            
            # ê´€ê³„ íƒ€ì…ì´ í‘œì¤€ì ì¸ì§€ í™•ì¸
            standard_relations = [
                'STUDIES', 'ANALYZES', 'COMPARES', 'USES', 'MEASURES', 'EVALUATES',
                'IMPLEMENTS', 'IS_TYPE_OF', 'PART_OF', 'CONTAINS', 'DESCRIBES',
                'REFERS_TO', 'RELATED_TO', 'SIMILAR_TO', 'DEPENDS_ON'
            ]
            
            if relation_type in standard_relations:
                return min(0.9, 0.5 + confidence * 0.4)
            else:
                # ë¹„í‘œì¤€ ê´€ê³„ íƒ€ì…ì˜ ê²½ìš° ë‚®ì€ ì ìˆ˜
                return min(0.6, confidence * 0.6)
                
        except Exception:
            return 0.4
    
    def _evaluate_relevance(self, relation: Dict, text: str, user_context: str) -> float:
        """ê´€ë ¨ì„± í‰ê°€"""
        try:
            # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ì™€ì˜ ê´€ë ¨ì„±
            if user_context:
                subject_entity = relation['subject']
                object_entity = relation['object']
                
                # ì»¨í…ìŠ¤íŠ¸ì— ì—”í‹°í‹°ë“¤ì´ ì–¸ê¸‰ë˜ëŠ”ì§€ í™•ì¸
                context_mentions = 0
                if subject_entity in user_context:
                    context_mentions += 1
                if object_entity in user_context:
                    context_mentions += 1
                
                if context_mentions == 2:
                    return 0.9
                elif context_mentions == 1:
                    return 0.7
            
            # confidence ê¸°ë°˜ ê´€ë ¨ì„±
            confidence = relation.get('confidence', 0.5)
            return min(0.8, 0.3 + confidence * 0.5)
            
        except Exception:
            return 0.5
    
    def _determine_quality_level(self, score: float) -> str:
        """ì ìˆ˜ ê¸°ë°˜ í’ˆì§ˆ ìˆ˜ì¤€ ê²°ì •"""
        for level, threshold in self.quality_thresholds.items():
            if score >= threshold:
                return level
        return 'poor'
    
    def _evaluate_with_llm(self, relations: List[Dict], text: str, 
                          entities: List[Dict], user_context: str,
                          relation_evaluations: List[Dict]) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•œ ì¶”ê°€ í‰ê°€"""
        try:
            evaluation_prompt = f"""
ë‹¤ìŒ ê´€ê³„ ì¶”ë¡  ê²°ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì›ë³¸ í…ìŠ¤íŠ¸**:
{text}

**ì¶”ë¡ ëœ ê´€ê³„ë“¤**:
{json.dumps(relations, ensure_ascii=False, indent=2)}

**ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸**: {user_context or "ì—†ìŒ"}

ê° ê´€ê³„ì— ëŒ€í•´ ë‹¤ìŒ í•­ëª©ì„ í‰ê°€í•´ì£¼ì„¸ìš”:
1. í…ìŠ¤íŠ¸ ê·¼ê±°ì˜ ì¶©ë¶„ì„± (1-10ì )
2. ê´€ê³„ íƒ€ì…ì˜ ì ì ˆì„± (1-10ì )
3. ì „ì²´ì ì¸ ìœ ìš©ì„± (1-10ì )

ë˜í•œ ë‹¤ìŒì„ ì œê³µí•´ì£¼ì„¸ìš”:
- ê°€ì¥ ì¢‹ì€ ê´€ê³„ 3ê°œ
- ê°€ì¥ ë¬¸ì œê°€ ìˆëŠ” ê´€ê³„ 3ê°œ
- ì „ì²´ì ì¸ í’ˆì§ˆ í‰ê°€
- ê°œì„  ì œì•ˆì‚¬í•­

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
    "overall_assessment": "ì „ì²´ í‰ê°€ ì½”ë©˜íŠ¸",
    "best_relations": ["ê´€ê³„1 ì„¤ëª…", "ê´€ê³„2 ì„¤ëª…", "ê´€ê³„3 ì„¤ëª…"],
    "problematic_relations": ["ë¬¸ì œ ê´€ê³„1", "ë¬¸ì œ ê´€ê³„2", "ë¬¸ì œ ê´€ê³„3"],
    "improvement_suggestions": ["ì œì•ˆ1", "ì œì•ˆ2", "ì œì•ˆ3"],
    "quality_score": 7.5
}}
```
"""
            
            messages = [{"role": "user", "content": evaluation_prompt}]
            response_generator = self.llm_engine.chat_completion(messages, stream=False)
            
            response_text = ""
            for response in response_generator:
                if response.get('type') == 'content':
                    response_text += response.get('text', '')
            
            # JSON íŒŒì‹±
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1)
                else:
                    # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ JSON ì°¾ê¸°
                    json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                    if json_match:
                        json_text = json_match.group(0)
                    else:
                        raise ValueError("No JSON found in LLM response")
                
                llm_eval = json.loads(json_text)
                return llm_eval
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"Failed to parse LLM evaluation response: {e}")
                return {
                    'error': 'Failed to parse LLM response',
                    'raw_response': response_text[:500]
                }
                
        except Exception as e:
            logger.warning(f"Failed to evaluate with LLM: {e}")
            return {'error': str(e)}
    
    def _generate_recommendations(self, relation_evaluations: List[Dict], 
                                overall_score: float, quality_level: str,
                                llm_evaluation: Dict) -> List[str]:
        """ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì „ì²´ í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
        if quality_level == 'poor':
            recommendations.append("ì „ì²´ì ì¸ ê´€ê³„ ì¶”ë¡  í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ê±°ë‚˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•´ë³´ì„¸ìš”.")
        elif quality_level == 'acceptable':
            recommendations.append("ê´€ê³„ ì¶”ë¡  í’ˆì§ˆì´ ë³´í†µ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ì¼ë¶€ ê´€ê³„ì˜ ê·¼ê±°ë¥¼ ë” ëª…í™•íˆ í•˜ë©´ ê°œì„ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
        elif quality_level == 'good':
            recommendations.append("ê´€ê³„ ì¶”ë¡  í’ˆì§ˆì´ ì¢‹ìŠµë‹ˆë‹¤. ì†Œìˆ˜ì˜ ê´€ê³„ë§Œ ê°œì„ í•˜ë©´ ë”ìš± ì™„ì„±ë„ê°€ ë†’ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤.")
        else:
            recommendations.append("ê´€ê³„ ì¶”ë¡  í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤!")
        
        # ê°œë³„ ê´€ê³„ ê¸°ë°˜ ì¶”ì²œ
        low_score_count = sum(1 for eval in relation_evaluations if eval['score'] < 0.5)
        if low_score_count > 0:
            recommendations.append(f"{low_score_count}ê°œì˜ ê´€ê³„ê°€ ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. í•´ë‹¹ ê´€ê³„ë“¤ì„ ê²€í† í•´ë³´ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ë¬¸ì œ íŒ¨í„´ í™•ì¸
        common_issues = {}
        for eval in relation_evaluations:
            for issue in eval['issues']:
                common_issues[issue] = common_issues.get(issue, 0) + 1
        
        if common_issues:
            most_common = max(common_issues.items(), key=lambda x: x[1])
            if most_common[1] > 1:
                recommendations.append(f"ê³µí†µ ë¬¸ì œ: {most_common[0]} (ì´ {most_common[1]}ê±´)")
        
        # LLM í‰ê°€ ê¸°ë°˜ ì¶”ì²œ
        if llm_evaluation and 'improvement_suggestions' in llm_evaluation:
            recommendations.extend(llm_evaluation['improvement_suggestions'])
        
        return recommendations[:10]  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ


class IterativeRelationInferenceAgent:
    """
    ë°˜ë³µì  ê´€ê³„ ì¶”ë¡  ê°œì„ ì„ ìœ„í•œ í†µí•© Agent
    RelationPromptAgentì™€ RelationEvaluatorAgentë¥¼ ì¡°í•©í•˜ì—¬ ê³ í’ˆì§ˆ ê´€ê³„ ì¶”ë¡ 
    """
    
    def __init__(self, llm_engine=None):
        self.llm_engine = llm_engine
        self.prompt_agent = RelationPromptAgent(llm_engine)
        self.evaluator_agent = RelationEvaluatorAgent(llm_engine)
        
        self.max_iterations = 3  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        self.target_score = 0.75  # ëª©í‘œ í’ˆì§ˆ ì ìˆ˜
    
    def infer_relations_iteratively(self, 
                                   text: str,
                                   entities: List[Dict],
                                   user_context: str = "",
                                   domain: str = "general",
                                   additional_guidelines: str = "",
                                   max_relations: int = 20) -> Dict[str, Any]:
        """
        ë°˜ë³µì  ê´€ê³„ ì¶”ë¡  ì‹¤í–‰
        
        Returns:
            ìµœì¢… ê´€ê³„ ì¶”ë¡  ê²°ê³¼
        """
        try:
            iteration_results = []
            best_result = None
            best_score = 0.0
            
            for iteration in range(self.max_iterations):
                logger.info(f"Starting relation inference iteration {iteration + 1}/{self.max_iterations}")
                
                # 1. í”„ë¡¬í”„íŠ¸ ìƒì„± (ì´ì „ ê²°ê³¼ ê¸°ë°˜ ê°œì„ )
                improvement_context = ""
                if iteration > 0 and iteration_results:
                    prev_evaluation = iteration_results[-1]['evaluation']
                    improvement_context = f"ì´ì „ ì‹œë„ì˜ ë¬¸ì œì ì„ ê°œì„ í•´ì£¼ì„¸ìš”: {', '.join(prev_evaluation['recommendations'][:3])}"
                
                prompt_result = self.prompt_agent.generate_relation_prompt(
                    text=text,
                    entities=entities,
                    user_context=user_context,
                    domain=domain,
                    additional_guidelines=additional_guidelines + "\n" + improvement_context,
                    max_relations=max_relations
                )
                
                if not prompt_result['success']:
                    continue
                
                # 2. LLMìœ¼ë¡œ ê´€ê³„ ì¶”ë¡ 
                relations = self._infer_relations_with_llm(
                    prompt_result['optimized_prompt'],
                    text,
                    entities
                )
                
                if not relations:
                    continue
                
                # 3. ê²°ê³¼ í‰ê°€
                evaluation_result = self.evaluator_agent.evaluate_relations(
                    relations=relations,
                    text=text,
                    entities=entities,
                    user_context=user_context,
                    domain=domain
                )
                
                if not evaluation_result['success']:
                    continue
                
                # 4. ê²°ê³¼ ì €ì¥
                iteration_result = {
                    'iteration': iteration + 1,
                    'prompt_result': prompt_result,
                    'relations': relations,
                    'evaluation': evaluation_result,
                    'score': evaluation_result['overall_score']
                }
                iteration_results.append(iteration_result)
                
                # 5. ìµœê³  ê²°ê³¼ ì—…ë°ì´íŠ¸
                current_score = evaluation_result['overall_score']
                if current_score > best_score:
                    best_result = iteration_result
                    best_score = current_score
                
                # 6. ëª©í‘œ ì ìˆ˜ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
                if current_score >= self.target_score:
                    logger.info(f"Target quality score {self.target_score} achieved at iteration {iteration + 1}")
                    break
                
                # 7. ê°œì„ ì´ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                if iteration > 0 and current_score <= iteration_results[-2]['score'] - 0.05:
                    logger.info(f"No significant improvement in iteration {iteration + 1}, stopping")
                    break
            
            # ìµœì¢… ê²°ê³¼ ìƒì„±
            if best_result:
                final_relations = best_result['evaluation']['approved_relations']
                
                return {
                    'success': True,
                    'final_relations': final_relations,
                    'best_iteration': best_result['iteration'],
                    'best_score': best_score,
                    'total_iterations': len(iteration_results),
                    'iteration_history': [
                        {
                            'iteration': r['iteration'],
                            'score': r['score'],
                            'relations_count': len(r['relations']),
                            'approved_count': len(r['evaluation']['approved_relations'])
                        }
                        for r in iteration_results
                    ],
                    'final_evaluation': best_result['evaluation'],
                    'improvement_achieved': best_score > (iteration_results[0]['score'] if iteration_results else 0),
                    'timestamp': datetime.now().isoformat()
                }
            else:
                return {
                    'success': False,
                    'error': 'No successful iterations completed',
                    'iteration_attempts': len(iteration_results),
                    'timestamp': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Error in iterative relation inference: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _infer_relations_with_llm(self, prompt: str, text: str, entities: List[Dict]) -> List[Dict]:
        """LLMì„ ì‚¬ìš©í•œ ê´€ê³„ ì¶”ë¡ """
        try:
            if not self.llm_engine:
                logger.warning("No LLM engine available for relation inference")
                return []
            
            messages = [{"role": "user", "content": prompt}]
            response_generator = self.llm_engine.chat_completion(messages, stream=False)
            
            response_text = ""
            for response in response_generator:
                if response.get('type') == 'content':
                    response_text += response.get('text', '')
            
            # JSON íŒŒì‹± ì‹œë„
            relations = self._parse_relations_response(response_text)
            
            # ê´€ê³„ ìœ íš¨ì„± ê²€ì¦
            valid_relations = []
            entity_ids = [e['id'] for e in entities]
            
            for relation in relations:
                if (isinstance(relation, dict) and 
                    'subject' in relation and 'type' in relation and 'object' in relation and
                    relation['subject'] in entity_ids and relation['object'] in entity_ids):
                    valid_relations.append(relation)
            
            logger.info(f"Successfully parsed {len(valid_relations)} valid relations from LLM response")
            return valid_relations
            
        except Exception as e:
            logger.error(f"Error in LLM relation inference: {e}")
            return []
    
    def _parse_relations_response(self, response_text: str) -> List[Dict]:
        """LLM ì‘ë‹µì—ì„œ ê´€ê³„ íŒŒì‹±"""
        try:
            # 1. JSON ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # 2. JSON ë°°ì—´ ì°¾ê¸°
                json_match = re.search(r'\[.*?\]', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                else:
                    raise ValueError("No JSON array found in response")
            
            # 3. JSON íŒŒì‹±
            relations = json.loads(json_text)
            
            if not isinstance(relations, list):
                raise ValueError("Response is not a JSON array")
            
            return relations
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"Failed to parse relations JSON: {e}")
            
            # 4. ëŒ€ì•ˆ: ì •ê·œì‹ìœ¼ë¡œ ê´€ê³„ íŒ¨í„´ ì°¾ê¸°
            try:
                return self._extract_relations_with_regex(response_text)
            except Exception as e2:
                logger.error(f"Failed to extract relations with regex: {e2}")
                return []
    
    def _extract_relations_with_regex(self, text: str) -> List[Dict]:
        """ì •ê·œì‹ì„ ì‚¬ìš©í•œ ê´€ê³„ ì¶”ì¶œ (fallback)"""
        relations = []
        
        # subject, type, object íŒ¨í„´ ì°¾ê¸°
        pattern = r'"subject"\s*:\s*"([^"]+)".*?"type"\s*:\s*"([^"]+)".*?"object"\s*:\s*"([^"]+)"'
        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            subject, rel_type, obj = match
            relations.append({
                'subject': subject.strip(),
                'type': rel_type.strip(),
                'object': obj.strip(),
                'confidence': 0.5,  # ê¸°ë³¸ê°’
                'explanation': 'Extracted via regex fallback'
            })
        
        return relations