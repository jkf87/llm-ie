#!/usr/bin/env python3
"""
Multi-Agent Knowledge Graph System Test Suite
LLM-IE ì›¹ì•±ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì§€ì‹ê·¸ë˜í”„ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸

ì´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ëŠ” ë‹¤ìŒ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ê²€ì¦í•©ë‹ˆë‹¤:
1. RelationPromptAgent - ê´€ê³„ ì¶”ë¡  í”„ë¡¬í”„íŠ¸ ìƒì„±
2. RelationEvaluatorAgent - ê´€ê³„ í’ˆì§ˆ í‰ê°€ 
3. IterativeRelationInferenceAgent - ë°˜ë³µì  ê´€ê³„ ê°œì„ 
4. KnowledgeGraphGenerator - ì§€ì‹ê·¸ë˜í”„ ìƒì„± (ì—…ë°ì´íŠ¸ëœ ë²„ì „)
5. KnowledgeGraphValidator - ì§€ì‹ê·¸ë˜í”„ ê²€ì¦ ë° ì˜¤ë¥˜ ìˆ˜ì •
6. API ì—”ë“œí¬ì¸íŠ¸ë“¤ - ìƒˆë¡œìš´ ë©€í‹°ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° API
"""

import sys
import os
import json
import time
import logging
from typing import Dict, List, Any

# ì›¹ì•± ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MockLLMEngine:
    """
    í…ŒìŠ¤íŠ¸ìš© Mock LLM Engine
    ì‹¤ì œ LLM API í˜¸ì¶œ ì—†ì´ ì˜ˆìƒë˜ëŠ” ì‘ë‹µì„ ì‹œë®¬ë ˆì´ì…˜
    """
    
    def __init__(self):
        self.call_count = 0
        
    def chat_completion(self, messages, stream=False):
        """Mock chat completion method"""
        self.call_count += 1
        
        # ë©”ì‹œì§€ ë‚´ìš©ì— ë”°ë¼ ë‹¤ë¥¸ ì‘ë‹µ ìƒì„±
        last_message = messages[-1]['content'] if messages else ''
        
        if 'prompt improvement' in last_message.lower() or 'í”„ë¡¬í”„íŠ¸' in last_message:
            # RelationPromptAgentìš© ì‘ë‹µ
            response = {
                "success": True,
                "improved_prompt": "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ë“¤ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”. íŠ¹íˆ ì—°êµ¬ ë…¼ë¬¸ì˜ ë§¥ë½ì—ì„œ...",
                "improvements_made": [
                    "ë„ë©”ì¸ íŠ¹í™” ì§€ì¹¨ ì¶”ê°€",
                    "ê´€ê³„ ìœ í˜• ëª…í™•í™”",
                    "ì˜ˆì‹œ ì¶”ê°€"
                ],
                "domain_guidelines": ["í•™ìˆ  ë…¼ë¬¸ì—ì„œëŠ” STUDIES, ANALYZES ê´€ê³„ê°€ ì¤‘ìš”", "ë°©ë²•ë¡ ê³¼ ê²°ê³¼ ê°„ì˜ USES ê´€ê³„ ì£¼ì˜"]
            }
            yield {'type': 'content', 'text': json.dumps(response, ensure_ascii=False)}
            
        elif 'evaluate' in last_message.lower() or 'í‰ê°€' in last_message:
            # RelationEvaluatorAgentìš© ì‘ë‹µ
            response = {
                "relevance_score": 8,
                "accuracy_score": 7,
                "consistency_score": 9,
                "specificity_score": 6,
                "overall_score": 7.5,
                "feedback": "ê´€ê³„ê°€ ëŒ€ì²´ë¡œ ì ì ˆí•˜ì§€ë§Œ ë” êµ¬ì²´ì ì¸ ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "suggestions": [
                    "ê´€ê³„ì˜ ë§¥ë½ì„ ë” ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”",
                    "ì—”í‹°í‹° ê°„ì˜ ì§ì ‘ì  ì—°ê´€ì„±ì„ ê°•í™”í•˜ì„¸ìš”"
                ]
            }
            yield {'type': 'content', 'text': json.dumps(response, ensure_ascii=False)}
            
        elif 'relation' in last_message.lower() or 'ê´€ê³„' in last_message:
            # ê´€ê³„ ì¶”ë¡ ìš© ì‘ë‹µ
            response = [
                {
                    "subject": "entity_1",
                    "relation_type": "STUDIES",
                    "object": "entity_2", 
                    "confidence": 0.85,
                    "explanation": "í…ìŠ¤íŠ¸ì—ì„œ entity_1ì´ entity_2ë¥¼ ì—°êµ¬í•œë‹¤ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤."
                },
                {
                    "subject": "entity_2",
                    "relation_type": "IS_TYPE_OF",
                    "object": "entity_3",
                    "confidence": 0.9,
                    "explanation": "entity_2ëŠ” entity_3ì˜ í•œ ì¢…ë¥˜ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤."
                }
            ]
            yield {'type': 'content', 'text': json.dumps(response, ensure_ascii=False)}
            
        else:
            # ê¸°ë³¸ ì‘ë‹µ
            yield {'type': 'content', 'text': '{"result": "mock response"}'}


class MultiAgentSystemTester:
    """Multi-Agent ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.mock_engine = MockLLMEngine()
        self.test_results = {}
        
        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°
        self.sample_frames = [
            {
                'frame_id': 'entity_1',
                'entity_text': 'ë”¥ëŸ¬ë‹',
                'start': 0,
                'end': 3,
                'attr': {'entity_type': 'TECHNOLOGY'}
            },
            {
                'frame_id': 'entity_2', 
                'entity_text': 'ìì—°ì–´ì²˜ë¦¬',
                'start': 10,
                'end': 15,
                'attr': {'entity_type': 'FIELD'}
            },
            {
                'frame_id': 'entity_3',
                'entity_text': 'BERT ëª¨ë¸',
                'start': 20,
                'end': 27,
                'attr': {'entity_type': 'MODEL'}
            }
        ]
        
        # RelationPromptAgentìš© ì—”í‹°í‹° í˜•ì‹
        self.sample_entities_for_prompt = [
            {
                'id': 'entity_1',
                'text': 'ë”¥ëŸ¬ë‹',
                'type': 'TECHNOLOGY'
            },
            {
                'id': 'entity_2',
                'text': 'ìì—°ì–´ì²˜ë¦¬',
                'type': 'FIELD'
            },
            {
                'id': 'entity_3',
                'text': 'BERT ëª¨ë¸',
                'type': 'MODEL'
            }
        ]
        
        self.sample_text = "ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ìì—°ì–´ì²˜ë¦¬ ì—°êµ¬ì—ì„œ BERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤."
        
    def test_relation_prompt_agent(self):
        """RelationPromptAgent í…ŒìŠ¤íŠ¸"""
        logger.info("Testing RelationPromptAgent...")
        
        try:
            from app.relation_agents import RelationPromptAgent
            
            agent = RelationPromptAgent(self.mock_engine)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
            result = agent.generate_relation_prompt(
                text=self.sample_text,
                entities=self.sample_entities_for_prompt,
                domain='research_paper',
                user_context='í•™ìˆ  ë…¼ë¬¸ ë¶„ì„',
                additional_guidelines='ì •í™•ì„±ì„ ì¤‘ì‹œí•˜ì„¸ìš”'
            )
            
            self.test_results['RelationPromptAgent'] = {
                'status': 'passed' if result.get('success') else 'failed',
                'details': result,
                'llm_calls': self.mock_engine.call_count
            }
            
            logger.info(f"RelationPromptAgent test result: {result.get('success', False)}")
            
        except Exception as e:
            self.test_results['RelationPromptAgent'] = {
                'status': 'error',
                'error': str(e)
            }
            logger.error(f"RelationPromptAgent test failed: {e}")
    
    def test_relation_evaluator_agent(self):
        """RelationEvaluatorAgent í…ŒìŠ¤íŠ¸"""
        logger.info("Testing RelationEvaluatorAgent...")
        
        try:
            from app.relation_agents import RelationEvaluatorAgent
            
            agent = RelationEvaluatorAgent(self.mock_engine)
            
            # ê´€ê³„ í‰ê°€ í…ŒìŠ¤íŠ¸ - ìƒˆë¡œìš´ ë©”ì„œë“œëª… ì‚¬ìš©
            sample_relations = [
                {
                    'subject': 'entity_1',
                    'relation_type': 'USED_IN',
                    'object': 'entity_2',
                    'confidence': 0.8,
                    'explanation': 'ë”¥ëŸ¬ë‹ì´ ìì—°ì–´ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤'
                }
            ]
            
            result = agent.evaluate_relations(
                relations=sample_relations,
                text=self.sample_text,
                entities=self.sample_frames,
                user_context='í•™ìˆ  ë…¼ë¬¸ ë¶„ì„'
            )
            
            self.test_results['RelationEvaluatorAgent'] = {
                'status': 'passed' if result.get('success') else 'failed',
                'details': result,
                'score': result.get('overall_score', 0)
            }
            
            logger.info(f"RelationEvaluatorAgent test result: {result.get('success', False)}, Score: {result.get('overall_score', 0)}")
            
        except Exception as e:
            self.test_results['RelationEvaluatorAgent'] = {
                'status': 'error',
                'error': str(e)
            }
            logger.error(f"RelationEvaluatorAgent test failed: {e}")
    
    def test_iterative_relation_inference_agent(self):
        """IterativeRelationInferenceAgent í…ŒìŠ¤íŠ¸"""
        logger.info("Testing IterativeRelationInferenceAgent...")
        
        try:
            from app.relation_agents import IterativeRelationInferenceAgent
            
            agent = IterativeRelationInferenceAgent(self.mock_engine)
            
            # ë°˜ë³µì  ê´€ê³„ ì¶”ë¡  í…ŒìŠ¤íŠ¸ - ìƒˆë¡œìš´ ë©”ì„œë“œëª… ì‚¬ìš©
            result = agent.infer_relations_iteratively(
                text=self.sample_text,
                entities=self.sample_entities_for_prompt,
                user_context='í•™ìˆ  ë…¼ë¬¸',
                domain='research_paper',
                additional_guidelines='ì •í™•í•œ ê´€ê³„ë§Œ ì¶”ì¶œí•˜ì„¸ìš”',
                max_relations=10
            )
            
            self.test_results['IterativeRelationInferenceAgent'] = {
                'status': 'passed' if result.get('success') else 'failed',
                'details': result,
                'relations_count': len(result.get('relations', [])),
                'iterations_used': result.get('iterations_used', 0)
            }
            
            logger.info(f"IterativeRelationInferenceAgent test result: {result.get('success', False)}")
            logger.info(f"Relations generated: {len(result.get('relations', []))}, Iterations: {result.get('iterations_used', 0)}")
            
        except Exception as e:
            self.test_results['IterativeRelationInferenceAgent'] = {
                'status': 'error', 
                'error': str(e)
            }
            logger.error(f"IterativeRelationInferenceAgent test failed: {e}")
    
    def test_knowledge_graph_generator(self):
        """KnowledgeGraphGenerator í…ŒìŠ¤íŠ¸ (ì—…ë°ì´íŠ¸ëœ ë²„ì „)"""
        logger.info("Testing KnowledgeGraphGenerator...")
        
        try:
            from app.knowledge_graph_agents import KnowledgeGraphGenerator
            
            generator = KnowledgeGraphGenerator(self.mock_engine)
            
            # ìƒ˜í”Œ ì¶”ì¶œ ë°ì´í„°
            extraction_data = {
                'doc_id': 'test_doc',
                'text': self.sample_text,
                'frames': self.sample_frames,
                'relations': []
            }
            
            # ì§€ì‹ê·¸ë˜í”„ ìƒì„± í…ŒìŠ¤íŠ¸ (ìƒˆ ë§¤ê°œë³€ìˆ˜ í¬í•¨)
            result = generator.generate_knowledge_graph(
                extraction_data=extraction_data,
                user_context='í•™ìˆ  ë…¼ë¬¸ ë¶„ì„',
                domain='research_paper',
                additional_guidelines='ì •í™•í•œ ê´€ê³„ ì¶”ë¡ ',
                max_relations=15,
                use_iterative_inference=True
            )
            
            self.test_results['KnowledgeGraphGenerator'] = {
                'status': 'passed' if result.get('success') else 'failed',
                'details': result,
                'triples_count': result.get('total_triples', 0),
                'has_rdf_formats': bool(result.get('rdf_formats'))
            }
            
            logger.info(f"KnowledgeGraphGenerator test result: {result.get('success', False)}")
            logger.info(f"Triples generated: {result.get('total_triples', 0)}")
            
        except Exception as e:
            self.test_results['KnowledgeGraphGenerator'] = {
                'status': 'error',
                'error': str(e)
            }
            logger.error(f"KnowledgeGraphGenerator test failed: {e}")
    
    def test_knowledge_graph_validator(self):
        """KnowledgeGraphValidator í…ŒìŠ¤íŠ¸ (ì˜¤ë¥˜ ìˆ˜ì • ê¸°ëŠ¥ í¬í•¨)"""
        logger.info("Testing KnowledgeGraphValidator...")
        
        try:
            from app.knowledge_graph_agents import KnowledgeGraphValidator, KnowledgeGraphGenerator
            
            # ë¨¼ì € í…ŒìŠ¤íŠ¸ìš© ì§€ì‹ê·¸ë˜í”„ ìƒì„±
            generator = KnowledgeGraphGenerator(self.mock_engine)
            extraction_data = {
                'doc_id': 'test_doc',
                'text': self.sample_text,
                'frames': self.sample_frames,
                'relations': []
            }
            kg_result = generator.generate_knowledge_graph(extraction_data)
            
            if kg_result.get('success'):
                validator = KnowledgeGraphValidator(self.mock_engine)
                
                # ê²€ì¦ í…ŒìŠ¤íŠ¸ (ìë™ ìˆ˜ì • í¬í•¨)
                validation_result = validator.validate_knowledge_graph(
                    kg_data=kg_result,
                    auto_fix_errors=True
                )
                
                self.test_results['KnowledgeGraphValidator'] = {
                    'status': 'passed' if validation_result.get('success') else 'failed',
                    'details': validation_result,
                    'overall_score': validation_result.get('overall_score', 0),
                    'errors_fixed': len(validation_result.get('errors_fixed', [])),
                    'auto_fix_enabled': validation_result.get('auto_fix_enabled', False)
                }
                
                logger.info(f"KnowledgeGraphValidator test result: {validation_result.get('success', False)}")
                logger.info(f"Validation score: {validation_result.get('overall_score', 0)}")
                logger.info(f"Errors fixed: {len(validation_result.get('errors_fixed', []))}")
            else:
                raise Exception("Failed to generate knowledge graph for validation test")
            
        except Exception as e:
            self.test_results['KnowledgeGraphValidator'] = {
                'status': 'error',
                'error': str(e)
            }
            logger.error(f"KnowledgeGraphValidator test failed: {e}")
    
    def test_api_integration(self):
        """API í†µí•© í…ŒìŠ¤íŠ¸ (Flask ì•± ì—†ì´ ëª¨ë“ˆ ë ˆë²¨ì—ì„œ)"""
        logger.info("Testing API integration...")
        
        try:
            # ì—¬ê¸°ì„œëŠ” API ì—”ë“œí¬ì¸íŠ¸ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ í…ŒìŠ¤íŠ¸
            # ì‹¤ì œ Flask ì•±ì„ ì‹¤í–‰í•˜ì§€ ì•Šê³  í•¨ìˆ˜ë§Œ í…ŒìŠ¤íŠ¸
            
            # Mock request ê°ì²´
            class MockRequest:
                def __init__(self, json_data):
                    self.json = json_data
            
            # ì´ê²ƒì€ ì‹¤ì œ Flask ì—†ì´ëŠ” ì™„ì „íˆ í…ŒìŠ¤íŠ¸í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ
            # ê¸°ë³¸ì ì¸ import í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰
            try:
                from app.routes import main_bp
                api_tests_passed = True
            except ImportError as ie:
                api_tests_passed = False
                api_error = str(ie)
            
            self.test_results['API_Integration'] = {
                'status': 'passed' if api_tests_passed else 'failed',
                'details': 'API endpoints imported successfully' if api_tests_passed else f'Import error: {api_error}',
                'endpoints_tested': [
                    '/api/relation-inference/iterative',
                    '/api/relations/edit', 
                    '/api/relations/validate-batch'
                ]
            }
            
            logger.info(f"API Integration test result: {api_tests_passed}")
            
        except Exception as e:
            self.test_results['API_Integration'] = {
                'status': 'error',
                'error': str(e)
            }
            logger.error(f"API Integration test failed: {e}")
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("Starting Multi-Agent System Test Suite...")
        
        start_time = time.time()
        
        # ê° ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        self.test_relation_prompt_agent()
        self.test_relation_evaluator_agent()
        self.test_iterative_relation_inference_agent()
        self.test_knowledge_graph_generator()
        self.test_knowledge_graph_validator()
        self.test_api_integration()
        
        end_time = time.time()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        self.print_test_summary(end_time - start_time)
        
        return self.test_results
    
    def print_test_summary(self, execution_time):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("MULTI-AGENT SYSTEM TEST SUMMARY")
        print("="*70)
        
        passed = 0
        failed = 0
        errors = 0
        
        for component, result in self.test_results.items():
            status = result['status']
            if status == 'passed':
                passed += 1
                status_symbol = "âœ…"
            elif status == 'failed':
                failed += 1  
                status_symbol = "âŒ"
            else:
                errors += 1
                status_symbol = "âš ï¸"
            
            print(f"{status_symbol} {component:<35} {status.upper()}")
            
            # ì¶”ê°€ ì„¸ë¶€ì‚¬í•­ ì¶œë ¥
            if 'score' in result:
                print(f"   â””â”€ Score: {result['score']}")
            if 'relations_count' in result:
                print(f"   â””â”€ Relations: {result['relations_count']}")
            if 'triples_count' in result:
                print(f"   â””â”€ Triples: {result['triples_count']}")
            if 'errors_fixed' in result:
                print(f"   â””â”€ Errors Fixed: {result['errors_fixed']}")
            if result['status'] == 'error':
                print(f"   â””â”€ Error: {result.get('error', 'Unknown error')}")
        
        print("-"*70)
        print(f"Total Tests: {len(self.test_results)}")
        print(f"âœ… Passed: {passed}")
        print(f"âŒ Failed: {failed}")
        print(f"âš ï¸  Errors: {errors}")
        print(f"Execution Time: {execution_time:.2f} seconds")
        print(f"Mock LLM Calls: {self.mock_engine.call_count}")
        print("="*70)
        
        # ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ
        if errors == 0 and failed == 0:
            print("ğŸ‰ ALL TESTS PASSED - Multi-Agent System is ready!")
        elif errors == 0:
            print("âš ï¸  Some tests failed - Review failed components")
        else:
            print("âŒ Critical errors found - System needs debugging")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("LLM-IE Multi-Agent Knowledge Graph System Test Suite")
    print("=" * 70)
    
    tester = MultiAgentSystemTester()
    results = tester.run_all_tests()
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    results_file = f"multiagent_test_results_{int(time.time())}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nDetailed test results saved to: {results_file}")
    
    # ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœ ë°˜í™˜
    all_passed = all(result['status'] == 'passed' for result in results.values())
    return all_passed


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)