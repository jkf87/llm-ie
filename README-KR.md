<div align="center"><img src=docs/readme_img/LLM-IE.png width=500 ></div>

![Python Version](https://img.shields.io/pypi/pyversions/llm-ie)
![PyPI](https://img.shields.io/pypi/v/llm-ie)
[![Paper](https://img.shields.io/badge/DOI-10.1093/jamiaopen/ooaf012-red)](https://doi.org/10.1093/jamiaopen/ooaf012)
[![Website](https://img.shields.io/badge/website-GitHub.io-purple)](https://daviden1013.github.io/llm-ie/)

**[English](README.md) | í•œêµ­ì–´**

LLM ê¸°ë°˜ ê°œì²´ëª… ì¸ì‹, ì†ì„± ì¶”ì¶œ ë° ê´€ê³„ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•˜ëŠ” í¬ê´„ì ì¸ íˆ´í‚·ì…ë‹ˆë‹¤.

| ê¸°ëŠ¥ | ì§€ì› |
|------|------|
| **í”„ë¡¬í”„íŠ¸ ì‘ì„±ì„ ìœ„í•œ LLM ì—ì´ì „íŠ¸** | :white_check_mark: ì›¹ ì•±, ëŒ€í™”í˜• ì±„íŒ… |
| **ê°œì²´ëª… ì¸ì‹ (NER)** | :white_check_mark: ì‚¬ìš©ì ì •ì˜ ê°€ëŠ¥í•œ ì„¸ë¶„í™” (ì˜ˆ: ë¬¸ì¥ ìˆ˜ì¤€, ë¬¸ì„œ ìˆ˜ì¤€) |
| **ê°œì²´ ì†ì„± ì¶”ì¶œ** | :white_check_mark: ìœ ì—°í•œ í˜•ì‹ |
| **ê´€ê³„ ì¶”ì¶œ (RE)** | :white_check_mark: ì´ì§„ ë° ë‹¤ì¤‘ í´ë˜ìŠ¤ ê´€ê³„ |
| **ì‹œê°í™”** | :white_check_mark: ì›¹ ì•±, ë‚´ì¥ ê°œì²´ ë° ê´€ê³„ ì‹œê°í™” |
| **ë°°ì¹˜ ì²˜ë¦¬** | :white_check_mark: Gemini Direct API, ë‹¤ì¤‘ API í‚¤ ì§€ì› |

## ğŸ†•ìµœê·¼ ì—…ë°ì´íŠ¸
- [v1.0.0](https://github.com/daviden1013/llm-ie/releases/tag/v1.0.0) (2025ë…„ 5ì›” 15ì¼): 
  - ğŸ“**ì‚¬ìš©ì ê°€ì´ë“œ**ê°€ [ë¬¸ì„œ í˜ì´ì§€](https://daviden1013.github.io/llm-ie/)ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
  - **ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜**ì´ *LLM-IE*ì— ë“œë˜ê·¸ ì•¤ ë“œë¡­ ì•¡ì„¸ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
  - ì²­í‚¹ ë°©ë²•(ì˜ˆ: ë¬¸ì¥)ê³¼ í”„ë¡¬í”„íŒ… ë°©ë²•(ì˜ˆ: ë¦¬ë·°)ì„ ë¶„ë¦¬í•˜ì—¬ **`FrameExtractor` ë¦¬íŒ©í† ë§**. ì²­í‚¹ì€ ì´ì œ `UnitChunker`ì™€ `ContextChunker`ì—ì„œ ì •ì˜ë˜ê³ , `FrameExtractor`ëŠ” í”„ë¡¬í”„íŒ… ë°©ë²•ì„ ì •ì˜í•©ë‹ˆë‹¤.
  - **ë¬¸ì„œ ì›¹ì‚¬ì´íŠ¸**. ì‚¬ìš©ì ê°€ì´ë“œì™€ API ì°¸ì¡°ê°€ ì´ì œ ì œê³µë©ë‹ˆë‹¤.
  - **ìµœì í™”ëœ ë™ì‹œ/ë°°ì¹˜ ì²˜ë¦¬**. ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ ë” ì˜ í™œìš©í•˜ê¸° ìœ„í•´ ì„¸ë§ˆí¬ì–´ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤.
- [v1.1.0](https://github.com/daviden1013/llm-ie/releases/tag/v1.1.0) (2025ë…„ 5ì›” 19ì¼): ì¶”ë¡  ëª¨ë¸(o3, Qwen3)ì„ ì§€ì›í•˜ëŠ” LLMë³„ êµ¬ì„±.
- [v1.2.0](https://github.com/daviden1013/llm-ie/releases/tag/v1.2.0) (2025ë…„ 6ì›” 15ì¼): ë³µì¡í•œ ì†ì„± ìŠ¤í‚¤ë§ˆë¥¼ ìœ„í•œ ì†ì„± ì¶”ì¶œê¸°.
- [v1.2.1](https://github.com/daviden1013/llm-ie/releases/tag/v1.2.1) (2025ë…„ 7ì›” 12ì¼): í”„ë¡¬í”„íŠ¸ ì—ë””í„°ì— ì±„íŒ… ê¸°ë¡ ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸° ê¸°ëŠ¥ ì¶”ê°€.
- [v1.2.2](https://github.com/daviden1013/llm-ie/releases/tag/v1.2.2) (2025ë…„ 8ì›” 25ì¼): ì¶”ë¡  LLM(GPT-OSS, Qwen3)ìš© êµ¬ì„± ì¶”ê°€.
- **ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€** (2025ë…„ 9ì›”): Gemini Direct API í†µí•© ë° ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ ì¶”ê°€.

## ğŸ“‘ëª©ì°¨
- [ê°œìš”](#ê°œìš”)
- [ì „ì œ ì¡°ê±´](#ì „ì œ-ì¡°ê±´)
- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
- [ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜](#ì›¹-ì• í”Œë¦¬ì¼€ì´ì…˜)
- [ë°°ì¹˜ ì²˜ë¦¬](#ë°°ì¹˜-ì²˜ë¦¬)
- [ì˜ˆì œ](#ì˜ˆì œ)
- [ìœ ìš©í•œ ìŠ¤í¬ë¦½íŠ¸](#ìœ ìš©í•œ-ìŠ¤í¬ë¦½íŠ¸)
- [ì‚¬ìš©ì ê°€ì´ë“œ](#ì‚¬ìš©ì-ê°€ì´ë“œ)
- [ë²¤ì¹˜ë§ˆí¬](#ë²¤ì¹˜ë§ˆí¬)
- [ì¸ìš©](#ì¸ìš©)

## âœ¨ê°œìš”
LLM-IEëŠ” ê°œì²´ëª…, ê°œì²´ ì†ì„± ë° ê°œì²´ ê´€ê³„ ì¶”ì¶œì„ ìœ„í•œ ê°•ë ¥í•œ ì •ë³´ ì¶”ì¶œ ìœ í‹¸ë¦¬í‹°ë¥¼ ì œê³µí•˜ëŠ” íˆ´í‚·ì…ë‹ˆë‹¤. ì•„ë˜ í”Œë¡œìš°ì°¨íŠ¸ëŠ” ì¼ë°˜ì ì¸ ì–¸ì–´ ìš”ì²­ë¶€í„° ì¶œë ¥ ì‹œê°í™”ê¹Œì§€ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

<div align="center"><img src="docs/readme_img/LLM-IE flowchart.png" width=800 ></div>

## ğŸš¦ì „ì œ ì¡°ê±´
ìµœì†Œ í•˜ë‚˜ì˜ LLM ì¶”ë¡  ì—”ì§„ì´ í•„ìš”í•©ë‹ˆë‹¤. ğŸš… [LiteLLM](https://github.com/BerriAI/litellm), ğŸ¦™ [Llama-cpp-python](https://github.com/abetlen/llama-cpp-python), <img src="docs/readme_img/ollama_icon.png" alt="Icon" width="22"/> [Ollama](https://github.com/ollama/ollama), ğŸ¤— [Huggingface_hub](https://github.com/huggingface/huggingface_hub), <img src=docs/readme_img/openai-logomark_white.png width=16 /> [OpenAI API](https://platform.openai.com/docs/api-reference/introduction), <img src=docs/readme_img/vllm-logo_small.png width=20 /> [vLLM](https://github.com/vllm-project/vllm), ê·¸ë¦¬ê³  **Gemini Direct API**ì— ëŒ€í•œ ë‚´ì¥ ì§€ì›ì´ ìˆìŠµë‹ˆë‹¤. ì„¤ì¹˜ ê°€ì´ë“œëŠ” í•´ë‹¹ í”„ë¡œì íŠ¸ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. ë‹¤ë¥¸ ì¶”ë¡  ì—”ì§„ì€ [InferenceEngine](src/llm_ie/engines.py) ì¶”ìƒ í´ë˜ìŠ¤ë¥¼ í†µí•´ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ’¿ì„¤ì¹˜
Python íŒ¨í‚¤ì§€ëŠ” PyPIì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```bash
pip install llm-ie 
```
ì´ íŒ¨í‚¤ì§€ëŠ” LLM ì¶”ë¡  ì—”ì§„ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì„¤ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì „ì œ ì¡°ê±´](#ì „ì œ-ì¡°ê±´) ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸš€ë¹ ë¥¸ ì‹œì‘
ChatGPTë¡œ í•©ì„±ëœ [ì˜ë£Œ ë…¸íŠ¸](demo/document/synthesized_note.txt)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ ì¶”ì¶œ ê³¼ì •ì„ ë°ëª¨í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì‘ì—…ì€ ì§„ë‹¨ëª…, ë²”ìœ„ ë° í•´ë‹¹ ì†ì„±(ì¦‰, ì§„ë‹¨ ë‚ ì§œ, ìƒíƒœ)ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### LLM ì¶”ë¡  ì—”ì§„ ì„ íƒ
ì•„ë˜ì˜ ë‚´ì¥ ì—”ì§„ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.

<details>
<summary>ğŸš… LiteLLM</summary>

```python
from llm_ie.engines import LiteLLMInferenceEngine

inference_engine = LiteLLMInferenceEngine(model="openai/Llama-3.3-70B-Instruct", base_url="http://localhost:8000/v1", api_key="EMPTY")
```
</details>

<details>
<summary><img src=docs/readme_img/openai-logomark_white.png width=16 /> OpenAI API ë° í˜¸í™˜ ì„œë¹„ìŠ¤</summary>

[API í‚¤ ì•ˆì „ì„ ìœ„í•œ ëª¨ë²” ì‚¬ë¡€](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)ë¥¼ ë”°ë¼ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
```python
from llm_ie.engines import OpenAIInferenceEngine

inference_engine = OpenAIInferenceEngine(model="gpt-4o-mini")
```

OpenAI í˜¸í™˜ ì„œë¹„ìŠ¤(ì˜ˆ: OpenRouter)ì˜ ê²½ìš°:
```python
from llm_ie.engines import OpenAIInferenceEngine

inference_engine = OpenAIInferenceEngine(base_url="https://openrouter.ai/api/v1", model="meta-llama/llama-4-scout")
```

</details>

<details>
<summary><img src=docs/readme_img/Azure_icon.png width=32 /> Azure OpenAI API</summary>

[Azure AI Services ë¹ ë¥¸ ì‹œì‘](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Ckeyless%2Ctypescript-keyless%2Cpython-new&pivots=programming-language-python)ì„ ë”°ë¼ ì—”ë“œí¬ì¸íŠ¸ì™€ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.

```python
from llm_ie.engines import AzureOpenAIInferenceEngine

inference_engine = AzureOpenAIInferenceEngine(model="gpt-4o-mini", 
                                              api_version="<your api version>")
```

</details>

<details>
<summary>ğŸ¤— Huggingface_hub</summary>

```python
from llm_ie.engines import HuggingFaceHubInferenceEngine

inference_engine = HuggingFaceHubInferenceEngine(model="meta-llama/Meta-Llama-3-8B-Instruct")
```
</details>

<details>
<summary><img src="docs/readme_img/ollama_icon.png" alt="Icon" width="22"/> Ollama</summary>

```python 
from llm_ie.engines import OllamaInferenceEngine

inference_engine = OllamaInferenceEngine(model_name="llama3.1:8b-instruct-q8_0")
```
</details>

<details>
<summary><img src=docs/readme_img/vllm-logo_small.png width=20 /> vLLM</summary>

vLLM ì§€ì›ì€ [OpenAI í˜¸í™˜ ì„œë²„](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html)ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ë” ë§ì€ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•´ì„œëŠ” ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

ì„œë²„ ì‹œì‘
```cmd
vllm serve meta-llama/Meta-Llama-3.1-8B-Instruct
```
ì¶”ë¡  ì—”ì§„ ì •ì˜
```python
from llm_ie.engines import OpenAIInferenceEngine
inference_engine = OpenAIInferenceEngine(base_url="http://localhost:8000/v1",
                                         api_key="EMPTY",
                                         model="meta-llama/Meta-Llama-3.1-8B-Instruct")
```
</details>

<details>
<summary>ğŸ¦™ Llama-cpp-python</summary>

```python
from llm_ie.engines import LlamaCppInferenceEngine

inference_engine = LlamaCppInferenceEngine(repo_id="bullerwins/Meta-Llama-3.1-8B-Instruct-GGUF",
                                           gguf_filename="Meta-Llama-3.1-8B-Instruct-Q8_0.gguf")
```
</details>

ì´ ë¹ ë¥¸ ì‹œì‘ ë°ëª¨ì—ì„œëŠ” OpenRouterë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì—ëŠ” Llama-4-Scoutë¥¼, ê°œì²´ ë° ì†ì„± ì¶”ì¶œì—ëŠ” Llama-3.1-70B-Instructë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
ë‹¤ë¥¸ ì¶”ë¡  ì—”ì§„, LLM ë˜ëŠ” ì–‘ìí™”ë¡œ ì¸í•´ ì¶œë ¥ì´ ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### LLM ì—ì´ì „íŠ¸ì™€ ì±„íŒ…í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
í”„ë¡¬í”„íŠ¸ ì—ë””í„° LLM ì—ì´ì „íŠ¸ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤. OpenRouter API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ `OPENROUTER_API_KEY`ì— ì €ì¥í•©ë‹ˆë‹¤.
```bash
export OPENROUTER_API_KEY=<OpenRouter API í‚¤>
```

```python
from llm_ie import OpenAIInferenceEngine, BasicLLMConfig, DirectFrameExtractor, PromptEditor, SentenceUnitChunker, SlideWindowContextChunker

# í”„ë¡¬í”„íŠ¸ ì—ë””í„°ë¥¼ ìœ„í•œ LLM ì¶”ë¡  ì—”ì§„ ì •ì˜
prompt_editor_llm = OpenAIInferenceEngine(base_url="https://openrouter.ai/api/v1", 
                                          model="meta-llama/llama-4-scout", 
                                          api_key=os.getenv("OPENROUTER_API_KEY"),
                                          config=BasicLLMConfig(temperature=0.4, 
                                                                max_new_tokens=4096))
# LLM í”„ë¡¬í”„íŠ¸ ì—ë””í„° ì •ì˜
editor = PromptEditor(prompt_editor_llm, DirectFrameExtractor)
# ì±„íŒ… ì‹œì‘
editor.chat()
```

ì´ê²ƒì€ ëŒ€í™”í˜• ì„¸ì…˜ì„ ì—½ë‹ˆë‹¤:
<div align="left"><img src=docs/readme_img/terminal_chat.PNG width=1000 ></div>

ì—ì´ì „íŠ¸ëŠ” ```DirectFrameExtractor```ì—ì„œ ìš”êµ¬í•˜ëŠ” ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‘ì„±í•©ë‹ˆë‹¤.
ëª‡ ë²ˆì˜ ì±„íŒ… í›„, ì‹œì‘í•  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì–»ìŠµë‹ˆë‹¤:

```
### ì‘ì—… ì„¤ëª…
ì•„ë˜ ë‹¨ë½ì—ëŠ” ì§„ë‹¨ ëª©ë¡ì´ í¬í•¨ëœ ì„ìƒ ë…¸íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì£¼ì˜ ê¹Šê²Œ ê²€í† í•˜ê³  ì§„ë‹¨ ë‚ ì§œì™€ ìƒíƒœë¥¼ í¬í•¨í•œ ì§„ë‹¨ì„ ì¶”ì¶œí•˜ì„¸ìš”.

### ìŠ¤í‚¤ë§ˆ ì •ì˜
ì¶œë ¥ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
    "entity_text"ëŠ” í…ìŠ¤íŠ¸ì— ë‚˜íƒ€ë‚˜ëŠ” ì§„ë‹¨ëª…,
    "Date"ëŠ” ì§„ë‹¨ì´ ë‚´ë ¤ì§„ ë‚ ì§œ,
    "Status"ëŠ” ì§„ë‹¨ì˜ í˜„ì¬ ìƒíƒœ(ì˜ˆ: í™œì„±, í•´ê²°ë¨ ë“±)

### ì¶œë ¥ í˜•ì‹ ì •ì˜
ì¶œë ¥ì€ JSON í˜•ì‹ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:
[
    {"entity_text": "<ì§„ë‹¨ëª…>", "attr": {"Date": "<YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ>", "Status": "<ìƒíƒœ>"}},
    {"entity_text": "<ì§„ë‹¨ëª…>", "attr": {"Date": "<YYYY-MM-DD í˜•ì‹ì˜ ë‚ ì§œ>", "Status": "<ìƒíƒœ>"}}
]

### ì¶”ê°€ íŒíŠ¸
- ì¶œë ¥ì€ ì œê³µëœ ë‚´ìš©ì„ 100% ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. ê°€ì§œ ì •ë³´ë¥¼ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- íŠ¹ì • ë‚ ì§œë‚˜ ìƒíƒœê°€ ì—†ëŠ” ê²½ìš° í•´ë‹¹ í‚¤ë¥¼ ìƒëµí•˜ì„¸ìš”.

### ì»¨í…ìŠ¤íŠ¸
ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” ì„ìƒ ë…¸íŠ¸ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤:
"{{input}}"
```

### ì •ë³´ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŒ… ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„
LLMì— ì „ì²´ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸í•˜ëŠ” ëŒ€ì‹ (ìš°ë¦¬ ì‹¤í—˜ì— ë”°ë¥´ë©´ ì„±ëŠ¥ì´ ë” ë‚˜ì©ë‹ˆë‹¤), ì…ë ¥ ë¬¸ì„œë¥¼ ë‹¨ìœ„(ì˜ˆ: ë¬¸ì¥, í…ìŠ¤íŠ¸ ì¤„, ë‹¨ë½)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. LLMì€ ë‹¤ìŒ ë‹¨ìœ„ë¡œ ì´ë™í•˜ê¸° ì „ì— í•œ ë²ˆì— í•˜ë‚˜ì˜ ë‹¨ìœ„ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤. ì´ëŠ” `UnitChunker` í´ë˜ìŠ¤ë¡œ ë‹¬ì„±ë©ë‹ˆë‹¤. ì´ ë°ëª¨ì—ì„œëŠ” ë¬¸ì¥ë³„ í”„ë¡¬í”„íŒ…ì„ ìœ„í•´ `SentenceUnitChunker`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. LLMì´ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë¬¸ì¥ì—ë§Œ ì§‘ì¤‘í•˜ì§€ë§Œ, ì´ ê²½ìš° 2ê°œ ë¬¸ì¥ì˜ ìŠ¬ë¼ì´ë“œ ìœˆë„ìš°ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” LLMì— ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” `SlideWindowContextChunker` í´ë˜ìŠ¤ë¡œ ë‹¬ì„±ë©ë‹ˆë‹¤. ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ ë” ë‚®ì€ ë¹„ìš©ì„ ìœ„í•´ ë” ì‘ì€ LLMì¸ llama-3.1-70b-instructë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¶œë ¥ ì•ˆì •ì„±ê³¼ ì¬í˜„ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ `temperature = 0.0`ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

```python
# í•©ì„±ëœ ì˜ë£Œ ë…¸íŠ¸ ë¡œë“œ
with open("./demo/document/synthesized_note.txt", 'r') as f:
    note_text = f.read()

# ì¶”ì¶œê¸°ë¥¼ ìœ„í•œ LLM ì¶”ë¡  ì—”ì§„ ì •ì˜
extractor_llm = OpenAIInferenceEngine(base_url="https://openrouter.ai/api/v1", 
                                      model="meta-llama/llama-3.1-70b-instruct", 
                                      api_key=os.getenv("OPENROUTER_API_KEY"),
                                      config=BasicLLMConfig(temperature=0.0, 
                                                            max_new_tokens=1024))
# ë‹¨ìœ„ ì²­ì»¤ ì •ì˜. ë¬¸ì¥ë³„ë¡œ í”„ë¡¬í”„íŠ¸í•©ë‹ˆë‹¤.
unit_chunker = SentenceUnitChunker()
# ì»¨í…ìŠ¤íŠ¸ ì²­ì»¤ ì •ì˜. ë‹¨ìœ„ì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
context_chunker = SlideWindowContextChunker(window_size=2)
# ì¶”ì¶œê¸° ì •ì˜
extractor = DirectFrameExtractor(inference_engine=extractor_llm, 
                                 unit_chunker=unit_chunker,
                                 context_chunker=context_chunker,
                                 prompt_template=prompt_template)
```

í”„ë ˆì„ ì¶”ì¶œì„ ì‹¤í–‰í•˜ë ¤ë©´ `extract_frames` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì†ì„±ì´ ìˆëŠ” ê°œì²´ ëª©ë¡("í”„ë ˆì„")ì´ ë°˜í™˜ë©ë‹ˆë‹¤. `concurrent=True`ë¡œ ì„¤ì •í•˜ì—¬ ë™ì‹œ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
```python
# ì¶”ì¶œ ê³¼ì •ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ë ¤ë©´ concurrent=False, stream=Trueë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:
frames = extractor.extract_frames(note_text, concurrent=False, verbose=True)
# ë” ë¹ ë¥¸ ì¶”ì¶œì„ ìœ„í•´ concurrent=Trueë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° í”„ë¡¬í”„íŒ…ì„ í™œì„±í™”í•˜ì„¸ìš”
# frames = extractor.extract_frames(note_text, concurrent=True)

# ì¶”ì¶œ í™•ì¸
for frame in frames:
    print(frame.to_dict())
```

### ë°ì´í„° ê´€ë¦¬ ë° ì‹œê°í™”
ë” ë‚˜ì€ ê´€ë¦¬ë¥¼ ìœ„í•´ í”„ë ˆì„ì„ ë¬¸ì„œ ê°ì²´ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œëŠ” ```text```ì™€ ```frames```ë¥¼ ë³´ìœ í•©ë‹ˆë‹¤. ```add_frame()``` ë©”ì„œë“œëŠ” ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•˜ê³  (í†µê³¼í•˜ë©´) ë¬¸ì„œì— í”„ë ˆì„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

```python
from llm_ie.data_types import LLMInformationExtractionDocument

# ë¬¸ì„œ ì •ì˜
doc = LLMInformationExtractionDocument(doc_id="í•©ì„±ëœ ì˜ë£Œ ë…¸íŠ¸",
                                       text=note_text)
# ë¬¸ì„œì— í”„ë ˆì„ ì¶”ê°€
doc.add_frames(frames, create_id=True)

# ë¬¸ì„œë¥¼ íŒŒì¼(.llmie)ì— ì €ì¥
doc.save("<íŒŒì¼ëª…>.llmie")
```

ì¶”ì¶œëœ í”„ë ˆì„ì„ ì‹œê°í™”í•˜ë ¤ë©´ ```viz_serve()``` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```python
doc.viz_serve()
```
Flask ì•±ì´ í¬íŠ¸ 5000(ê¸°ë³¸ê°’)ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤.

<div align="left"><img src="docs/readme_img/llm-ie_demo.PNG" width=1000 ></div>

## ğŸŒì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
*LLM-IE*ì— ì½”ë“œ ì—†ì´ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆëŠ” ë“œë˜ê·¸ ì•¤ ë“œë¡­ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.

### ì„¤ì¹˜
ì´ë¯¸ì§€ëŠ” ğŸ³Docker Hubì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ë¡œ í’€í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”:
```bash
docker pull daviden1013/llm-ie-web-app:latest
docker run -p 5000:5000 daviden1013/llm-ie-web-app:latest
```

### ê¸°ëŠ¥
í”„ë¡¬í”„íŠ¸ ì—ë””í„° LLM ì—ì´ì „íŠ¸ì™€ ì±„íŒ…í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤.
![web_app_prompt_editor](docs/readme_img/web_app_prompt_editor.PNG)

ìŠ¤íŠ¸ë¦¼ í”„ë ˆì„ ì¶”ì¶œ ë° ì¶œë ¥ ë‹¤ìš´ë¡œë“œ.
![web_app_frame_extractor](docs/readme_img/web_app_frame_extraction.PNG)

## ğŸš€ë°°ì¹˜ ì²˜ë¦¬
LLM-IEëŠ” ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³ ê¸‰ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### Gemini Direct API ë°°ì¹˜ ì²˜ë¦¬
ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Googleì˜ Gemini APIë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” íŠ¹ë³„í•œ ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

#### ì£¼ìš” ê¸°ëŠ¥:
- **ë‹¤ì¤‘ API í‚¤ ì§€ì›**: ì—¬ëŸ¬ API í‚¤ë¥¼ ìˆœí™˜ ì‚¬ìš©í•˜ì—¬ ì†ë„ ì œí•œ íšŒí”¼
- **ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬**: í° í…ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬ ê°€ëŠ¥í•œ ì²­í¬ë¡œ ë¶„í• 
- **ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©**: ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ìƒí™© í™•ì¸
- **êµ¬ì„± ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜**: ì²­í¬ í¬ê¸°, ì¤‘ë³µ, ë°°ì¹˜ í¬ê¸°, ì§€ì—° ì‹œê°„ ë“± ì¡°ì • ê°€ëŠ¥
- **ì˜¤ë¥˜ ì²˜ë¦¬**: ê°•ë ¥í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

#### ì›¹ UIì—ì„œ ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš©ë²•:
1. **ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”**: í”„ë ˆì„ ì¶”ì¶œ í˜ì´ì§€ì—ì„œ "ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™”" ì²´í¬ë°•ìŠ¤ ì„ íƒ
2. **ë°°ì¹˜ ìœ í˜• ì„ íƒ**: "Gemini Direct" ë˜ëŠ” "LLM-IE í˜¸í™˜" ì¤‘ ì„ íƒ
3. **API í‚¤ ì„¤ì •**: ì—¬ëŸ¬ API í‚¤ë¥¼ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥
4. **ë§¤ê°œë³€ìˆ˜ êµ¬ì„±**:
   - **ì²­í¬ í¬ê¸°**: ê° ì²­í¬ì˜ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 1000)
   - **ì¤‘ë³µ í¬ê¸°**: ì²­í¬ ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 100)
   - **ë°°ì¹˜ í¬ê¸°**: ë™ì‹œ ì²˜ë¦¬í•  ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 5)
   - **ë°°ì¹˜ ê°„ ì§€ì—°**: ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) (ê¸°ë³¸ê°’: 2.0)

#### ì˜ˆì‹œ êµ¬ì„±:
```
API í‚¤ë“¤:
AIzaSyC1234567890abcdef...
AIzaSyD0987654321fedcba...

ì²­í¬ í¬ê¸°: 1500
ì¤‘ë³µ í¬ê¸°: 150
ë°°ì¹˜ í¬ê¸°: 3
ë°°ì¹˜ ê°„ ì§€ì—°: 1.5ì´ˆ
```

### í”„ë¡œê·¸ë˜ë° ë°©ì‹ ë°°ì¹˜ ì²˜ë¦¬
Python ì½”ë“œì—ì„œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:

```python
from web_app.app.batch_services import BatchProcessor
from web_app.app.gemini_direct_engine import GeminiDirectBatchProcessor

# LLM-IE í˜¸í™˜ ë°°ì¹˜ ì²˜ë¦¬
llm_config = {"api_type": "openai_compatible", "model": "gpt-4"}
api_keys = ["key1", "key2", "key3"]
batch_processor = BatchProcessor(llm_config, api_keys)

# Gemini Direct ë°°ì¹˜ ì²˜ë¦¬
gemini_config = {
    'gemini_model': 'gemini-2.0-flash',
    'temperature': 0.2,
    'max_tokens': 4096
}
gemini_processor = GeminiDirectBatchProcessor(api_keys, gemini_config)
```

### ì„±ëŠ¥ ìµœì í™”
- **API í‚¤ ìˆœí™˜**: ì†ë„ ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ í‚¤ ì‚¬ìš©
- **ì²­í¬ í¬ê¸° ì¡°ì •**: ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì— ë§ê²Œ ì¡°ì •
- **ë°°ì¹˜ í¬ê¸° ìµœì í™”**: ì²˜ë¦¬ ì†ë„ì™€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê· í˜•
- **ì§€ì—° ì‹œê°„ ì„¤ì •**: API ì œí•œì„ ì¤€ìˆ˜í•˜ë„ë¡ ë°°ì¹˜ ê°„ ì§€ì—° ì¡°ì •

## ğŸ“˜ì˜ˆì œ
- [LLM í”„ë¡¬í”„íŠ¸ ì—ë””í„°ì™€ ëŒ€í™”í˜• ì±„íŒ…](demo/prompt_template_writing_via_chat.ipynb)
- [LLM í”„ë¡¬í”„íŠ¸ ì—ë””í„°ë¡œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±](demo/prompt_template_writing.ipynb)
- [ì•½ë¬¼, ê°•ë„, ë¹ˆë„ì— ëŒ€í•œ NER + RE](demo/medication_relation_extraction.ipynb)

## ğŸ”§ìœ ìš©í•œ ìŠ¤í¬ë¦½íŠ¸
ë§ì€ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…œí”Œë¦¿ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ [ì—¬ê¸°](package/llm-ie/pipelines/)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš© ì‚¬ë¡€ì— ë”°ë¼ ìˆ˜ì •í•˜ì„¸ìš”.
- [ë§ì€ ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬](package/llm-ie/pipelines/sequential_frame_extraction.py)
- [ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ë§ì€ ë¬¸ì„œ ì²˜ë¦¬](package/llm-ie/pipelines/multithread_frame_extraction.py)

## ğŸ“ì‚¬ìš©ì ê°€ì´ë“œ
ìì„¸í•œ ì‚¬ìš©ì ê°€ì´ë“œëŠ” ìš°ë¦¬ì˜ [ë¬¸ì„œ í˜ì´ì§€](https://daviden1013.github.io/llm-ie/)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“Šë²¤ì¹˜ë§ˆí¬
ìƒì˜í•™ ì •ë³´ ì¶”ì¶œ ì‘ì—…ì—ì„œ í”„ë ˆì„ ë° ê´€ê³„ ì¶”ì¶œê¸°ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì™€ ì‹¤í—˜ ì½”ë“œëŠ” [ì´ í˜ì´ì§€](https://github.com/daviden1013/LLM-IE_Benchmark)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ“ì¸ìš©
ë” ë§ì€ ì •ë³´ì™€ ë²¤ì¹˜ë§ˆí¬ëŠ” ìš°ë¦¬ì˜ ë…¼ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”:
```bibtex
@article{hsu2025llm,
  title={LLM-IE: a python package for biomedical generative information extraction with large language models},
  author={Hsu, Enshuo and Roberts, Kirk},
  journal={JAMIA open},
  volume={8},
  number={2},
  pages={ooaf012},
  year={2025},
  publisher={Oxford University Press}
}
```